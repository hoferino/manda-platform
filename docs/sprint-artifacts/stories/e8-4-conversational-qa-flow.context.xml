<story-context id="bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>E8</epicId>
    <storyId>E8.4</storyId>
    <title>Conversational Q&A Suggestion Flow</title>
    <status>drafted</status>
    <generatedAt>2025-12-09</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/stories/e8-4-conversational-qa-flow.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>analyst</asA>
    <iWant>the AI to suggest Q&A items when it can't resolve a gap</iWant>
    <soThat>I capture questions for the client at the right moment during document analysis</soThat>
    <tasks>
      <task id="1" title="Enhance System Prompt with Q&A Suggestion Behavior">
        <subtasks>
          <subtask>1.1 Add "Q&A Suggestion Flow" section to lib/agent/prompts.ts AGENT_SYSTEM_PROMPT</subtask>
          <subtask>1.2 Define clear triggers for Q&A suggestion (KB miss, contradictions that can't be resolved, incomplete information)</subtask>
          <subtask>1.3 Add guidance on drafting question text from conversation context</subtask>
          <subtask>1.4 Add explicit requirement for user confirmation before calling add_qa_item</subtask>
          <subtask>1.5 Add examples of good/bad Q&A suggestion flows</subtask>
        </subtasks>
        <ac>AC#1, AC#2, AC#3, AC#4</ac>
      </task>
      <task id="2" title="Update TOOL_USAGE_PROMPT for add_qa_item">
        <subtasks>
          <subtask>2.1 Add Q&A-specific tool usage guidance in TOOL_USAGE_PROMPT</subtask>
          <subtask>2.2 Document when to use add_qa_item (after user confirms) vs suggest_questions (for exploration)</subtask>
          <subtask>2.3 Include category mapping guidance (query topic -> QA category)</subtask>
        </subtasks>
        <ac>AC#1, AC#2</ac>
      </task>
      <task id="3" title="Add Q&A Category Inference Helper">
        <subtasks>
          <subtask>3.1 Create inferQACategoryFromQuery() utility in lib/agent/utils/qa-category.ts</subtask>
          <subtask>3.2 Map query keywords to categories (churn -> Operations, revenue/cost/margin -> Financials, contract -> Legal, etc.)</subtask>
          <subtask>3.3 Export from lib/agent/utils/index.ts</subtask>
          <subtask>3.4 Write unit tests for category inference</subtask>
        </subtasks>
        <ac>AC#2</ac>
      </task>
      <task id="4" title="Add Q&A Question Drafting Utility">
        <subtasks>
          <subtask>4.1 Create draftQAQuestion() utility in lib/agent/utils/qa-question.ts</subtask>
          <subtask>4.2 Accept query text, topic, and optional time period/context</subtask>
          <subtask>4.3 Format as professional client-facing question</subtask>
          <subtask>4.4 Write unit tests for question drafting</subtask>
        </subtasks>
        <ac>AC#2</ac>
      </task>
      <task id="5" title="Write Prompt Tests">
        <subtasks>
          <subtask>5.1 Create lib/agent/prompts.test.ts if not exists</subtask>
          <subtask>5.2 Test that system prompt includes Q&A suggestion guidance</subtask>
          <subtask>5.3 Test that TOOL_USAGE_PROMPT includes add_qa_item usage</subtask>
          <subtask>5.4 Verify prompt includes user confirmation requirement</subtask>
        </subtasks>
        <ac>AC#1, AC#2, AC#3, AC#4</ac>
      </task>
      <task id="6" title="Create E2E Test Scenarios">
        <subtasks>
          <subtask>6.1 Create e2e/qa-suggestion-flow.spec.ts for Playwright E2E tests</subtask>
          <subtask>6.2 Test scenario: User asks about missing data -> AI suggests Q&A -> User confirms -> Q&A created</subtask>
          <subtask>6.3 Test scenario: User asks about missing data -> AI suggests Q&A -> User declines -> No Q&A created</subtask>
          <subtask>6.4 Test that Q&A item appears in Q&A list after creation</subtask>
        </subtasks>
        <ac>AC#1, AC#3, AC#4</ac>
      </task>
      <task id="7" title="Integration Test with Mocked LLM">
        <subtasks>
          <subtask>7.1 Create integration test for Q&A suggestion flow</subtask>
          <subtask>7.2 Mock LLM responses for KB miss -> suggestion -> confirmation</subtask>
          <subtask>7.3 Verify add_qa_item tool is called with correct parameters</subtask>
          <subtask>7.4 Verify correct category is inferred from query</subtask>
        </subtasks>
        <ac>All ACs</ac>
      </task>
      <task id="8" title="Verify Build and Types">
        <subtasks>
          <subtask>8.1 Run npm run type-check - no errors</subtask>
          <subtask>8.2 Run unit tests for new utilities</subtask>
          <subtask>8.3 Run build to ensure no compilation errors</subtask>
        </subtasks>
        <ac>All ACs</ac>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC1">When the agent's query_knowledge_base tool returns no relevant findings for a user query, the agent detects this as a gap and conversationally offers to add the missing information to the Q&A list</criterion>
    <criterion id="AC2">The AI drafts a well-formed question based on the conversation context, including relevant details (time period, specific metrics, etc.)</criterion>
    <criterion id="AC3">The Q&A item is only created after explicit user confirmation (e.g., "Yes", "Yes, add it", "Sure")</criterion>
    <criterion id="AC4">If the user declines (e.g., "No", "I'll ask differently"), the conversation continues without adding to Q&A</criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>docs/sprint-artifacts/tech-spec-epic-E8.md</path>
        <title>Epic 8 Technical Specification</title>
        <section>E8.4: Conversational Q&A Flow, Conversational Q&A Suggestion Flow</section>
        <snippet>Defines the workflow: KB miss -> agent suggests Q&A -> user confirms -> add_qa_item called. AC-8.4.1 through AC-8.4.4 specify gap detection, question drafting, and confirmation requirements.</snippet>
      </doc>
      <doc>
        <path>docs/epics.md</path>
        <title>Epic Definitions</title>
        <section>Story E8.4: Conversational Q&A Suggestion Flow</section>
        <snippet>As an analyst, I want the AI to suggest Q&A items when it can't resolve a gap, So that I capture questions for the client at the right moment.</snippet>
      </doc>
      <doc>
        <path>docs/agent-behavior-spec.md</path>
        <title>Agent Behavior Specification</title>
        <section>P2 - Response Formatting</section>
        <snippet>Never show confidence scores. When information is missing, explain WHY and offer next step: "Would you like me to add this to the Q&A list?"</snippet>
      </doc>
      <doc>
        <path>docs/sprint-artifacts/stories/e8-3-agent-tool-add-qa-item.md</path>
        <title>Story E8.3 - add_qa_item Tool Implementation</title>
        <section>Completion Notes, Tool Registration Pattern</section>
        <snippet>addQAItemTool is registered as 17th tool. Schema validates question (10-2000 chars), category (6 enums), priority. Uses createQAItem from lib/services/qa.ts.</snippet>
      </doc>
    </docs>
    <code>
      <file>
        <path>manda-app/lib/agent/prompts.ts</path>
        <kind>prompts</kind>
        <symbol>AGENT_SYSTEM_PROMPT, TOOL_USAGE_PROMPT, getSystemPrompt</symbol>
        <lines>1-267</lines>
        <reason>PRIMARY FILE TO MODIFY. Add Q&A suggestion flow guidance to AGENT_SYSTEM_PROMPT (after "Proactive Suggestions" section) and update TOOL_USAGE_PROMPT with add_qa_item usage.</reason>
      </file>
      <file>
        <path>manda-app/lib/agent/tools/qa-tools.ts</path>
        <kind>tool</kind>
        <symbol>addQAItemTool, AddQAItemInputSchema</symbol>
        <lines>1-128</lines>
        <reason>The add_qa_item tool that will be called after user confirms Q&A suggestion. Reference for tool name and parameter format.</reason>
      </file>
      <file>
        <path>manda-app/lib/agent/tools/all-tools.ts</path>
        <kind>registry</kind>
        <symbol>allChatTools, TOOL_CATEGORIES, TOOL_COUNT</symbol>
        <lines>1-158</lines>
        <reason>Tool registry showing 17 tools. add_qa_item is tool #11, in qa category. TOOL_USAGE_PROMPT should match descriptions here.</reason>
      </file>
      <file>
        <path>manda-app/lib/agent/schemas.ts</path>
        <kind>schemas</kind>
        <symbol>AddQAItemInputSchema, QACategorySchema, QAPrioritySchema</symbol>
        <lines>267-298</lines>
        <reason>Schema definition for add_qa_item tool input. Shows required fields: dealId, question, category, priority, sourceFindingId (optional).</reason>
      </file>
      <file>
        <path>manda-app/lib/types/qa.ts</path>
        <kind>types</kind>
        <symbol>QACategory, QAPriority, QACategorySchema, QAPrioritySchema</symbol>
        <lines>1-100</lines>
        <reason>Q&A type definitions. QACategory enum: 'Financials' | 'Legal' | 'Operations' | 'Market' | 'Technology' | 'HR'. Used for category inference mapping.</reason>
      </file>
      <file>
        <path>manda-app/lib/agent/tools/knowledge-tools.ts</path>
        <kind>tool</kind>
        <symbol>queryKnowledgeBaseTool</symbol>
        <reason>The query_knowledge_base tool that triggers Q&A suggestion when it returns no results. Understanding its output format helps define suggestion trigger.</reason>
      </file>
      <file>
        <path>manda-app/e2e/data-room.spec.ts</path>
        <kind>test</kind>
        <symbol>Playwright E2E tests</symbol>
        <reason>Reference for E2E test patterns. Shows auth setup, page navigation, and test structure.</reason>
      </file>
    </code>
    <dependencies>
      <npm>
        <package name="vitest" version="^2.x">Unit test framework for prompt tests</package>
        <package name="@playwright/test" version="^1.x">E2E testing framework for Q&A suggestion flow tests</package>
        <package name="@langchain/core" version="^0.3.x">LangChain core - no changes needed, just reference for tool patterns</package>
      </npm>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="prompt-engineering">This story is primarily prompt engineering - enhancing AGENT_SYSTEM_PROMPT to trigger Q&A suggestion flow. No new tools needed.</constraint>
    <constraint type="confirmation-required">Agent MUST ask for and receive explicit user confirmation before calling add_qa_item. Enforced through prompt instructions.</constraint>
    <constraint type="category-inference">Agent should infer QA category from query context. Utility functions can help but LLM makes final decision.</constraint>
    <constraint type="question-quality">Drafted questions should be: Specific (include time periods, metrics), Professional (client-facing language), Actionable (ask for concrete data).</constraint>
    <constraint type="no-scores">Never show confidence scores to user (P2 compliance). Use natural language explanations.</constraint>
    <constraint type="existing-tool">Use existing add_qa_item tool from E8.3 - do not create new tools.</constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>add_qa_item</name>
      <kind>agent tool</kind>
      <signature>add_qa_item({ dealId: string, question: string, category: QACategory, priority: QAPriority, sourceFindingId?: string })</signature>
      <path>manda-app/lib/agent/tools/qa-tools.ts</path>
    </interface>
    <interface>
      <name>query_knowledge_base</name>
      <kind>agent tool</kind>
      <signature>query_knowledge_base({ query: string, filters?: object, limit?: number })</signature>
      <path>manda-app/lib/agent/tools/knowledge-tools.ts</path>
    </interface>
    <interface>
      <name>AGENT_SYSTEM_PROMPT</name>
      <kind>constant</kind>
      <signature>const AGENT_SYSTEM_PROMPT: string</signature>
      <path>manda-app/lib/agent/prompts.ts</path>
    </interface>
    <interface>
      <name>TOOL_USAGE_PROMPT</name>
      <kind>constant</kind>
      <signature>const TOOL_USAGE_PROMPT: string</signature>
      <path>manda-app/lib/agent/prompts.ts</path>
    </interface>
    <interface>
      <name>QACategory</name>
      <kind>type</kind>
      <signature>'Financials' | 'Legal' | 'Operations' | 'Market' | 'Technology' | 'HR'</signature>
      <path>manda-app/lib/types/qa.ts</path>
    </interface>
  </interfaces>

  <tests>
    <standards>
      Use Vitest for unit tests of prompt content and utility functions. Use Playwright for E2E tests of the full Q&A suggestion flow. Prompt tests verify string content includes required guidance. E2E tests use mocked LLM responses to verify tool invocation flow.
    </standards>
    <locations>
      <location>manda-app/lib/agent/prompts.test.ts (create new)</location>
      <location>manda-app/lib/agent/utils/qa-category.test.ts (create new)</location>
      <location>manda-app/lib/agent/utils/qa-question.test.ts (create new)</location>
      <location>manda-app/e2e/qa-suggestion-flow.spec.ts (create new)</location>
    </locations>
    <ideas>
      <idea ac="AC1">Test AGENT_SYSTEM_PROMPT includes "Q&A Suggestion Flow" section</idea>
      <idea ac="AC1">Test prompt includes trigger conditions: KB miss, contradictions, incomplete info</idea>
      <idea ac="AC2">Test prompt includes question drafting guidance with time period, specificity requirements</idea>
      <idea ac="AC2">Test TOOL_USAGE_PROMPT includes add_qa_item usage with category mapping</idea>
      <idea ac="AC3">Test prompt explicitly requires user confirmation before add_qa_item call</idea>
      <idea ac="AC3">Test prompt includes confirmation phrase examples: "Yes", "Add it", "Sure"</idea>
      <idea ac="AC4">Test prompt includes decline handling: continue conversation without Q&A</idea>
      <idea ac="AC2">Test inferQACategoryFromQuery maps "churn" to "Operations"</idea>
      <idea ac="AC2">Test inferQACategoryFromQuery maps "revenue" to "Financials"</idea>
      <idea ac="AC2">Test inferQACategoryFromQuery maps "contract" to "Legal"</idea>
      <idea ac="AC2">Test draftQAQuestion adds time period context when provided</idea>
      <idea ac="AC2">Test draftQAQuestion produces professional client-facing format</idea>
      <idea>E2E: KB miss -> suggestion -> user confirms "Yes" -> Q&A item created</idea>
      <idea>E2E: KB miss -> suggestion -> user declines "No" -> no Q&A item</idea>
      <idea>E2E: Q&A item appears in /projects/[id]/qa after creation</idea>
    </ideas>
  </tests>

  <categoryMapping>
    <mapping query="revenue, costs, margins, EBITDA, financials, profit, loss" category="Financials"/>
    <mapping query="contracts, agreements, IP, compliance, legal, lawsuit, litigation" category="Legal"/>
    <mapping query="customers, churn, operations, processes, supply chain" category="Operations"/>
    <mapping query="competition, market share, positioning, competitors" category="Market"/>
    <mapping query="tech stack, systems, integrations, software, API" category="Technology"/>
    <mapping query="team, employees, org structure, headcount, HR, hiring" category="HR"/>
  </categoryMapping>

  <exampleFlow>
    <step num="1">User: "What's the customer churn rate?"</step>
    <step num="2">Agent: query_knowledge_base("customer churn rate")</step>
    <step num="3">Agent: [No relevant findings found]</step>
    <step num="4">Agent: "I couldn't find churn rate data in the uploaded documents. This seems like important information for the deal analysis. Should I add this to your Q&A list for the client?"</step>
    <step num="5">User: "Yes, add it"</step>
    <step num="6">Agent: add_qa_item(question="What is the historical customer churn rate (monthly/annual) for the past 3 years?", category="Operations", priority="high")</step>
    <step num="7">Agent: "Added to Q&A list: 'What is the historical customer churn rate...' under Operations category (High priority). You now have 23 questions in your Q&A list."</step>
  </exampleFlow>
</story-context>
