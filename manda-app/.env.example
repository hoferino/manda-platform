# Manda M&A Intelligence Platform - Environment Variables
# Copy this file to .env.local and fill in your values

# ===================
# Supabase Configuration (Story E1.2)
# ===================
# Get these from your Supabase project: Settings > API
NEXT_PUBLIC_SUPABASE_URL=https://your-project.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-anon-key

# Server-only - NEVER expose to client
# Get from Supabase: Settings > API > service_role key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# ===================
# Database Configuration (Story E1.3)
# ===================
# Direct PostgreSQL connection (for migrations/scripts)
DATABASE_URL=postgresql://user:password@db.your-project.supabase.co:5432/postgres

# ===================
# Neo4j Configuration (Story E1.7)
# ===================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-password

# ===================
# pg-boss Configuration (Story E1.8)
# ===================
# Schema name for pg-boss tables (default: pgboss)
PGBOSS_SCHEMA=pgboss
# Maximum concurrent jobs per worker (default: 5)
PGBOSS_CONCURRENCY=5

# ===================
# Google Cloud Storage (Story E2.1)
# ===================
# GCP Project ID for GCS
GCS_PROJECT_ID=your-gcp-project-id
# Default bucket name for document storage
GCS_BUCKET_NAME=manda-documents
# Option 1: Path to service account JSON file (local development)
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
# Option 2: Service account JSON as string (production deployment)
# GCS_CREDENTIALS_JSON={"type":"service_account","project_id":"..."}

# ===================
# LLM Configuration (Story E5.1)
# ===================
# Provider Selection: anthropic | openai | google (default: anthropic)
LLM_PROVIDER=anthropic

# Model Selection (optional, uses provider default if not set)
# Anthropic: claude-sonnet-4-5-20250929, claude-3-opus-20240229, claude-3-haiku-20240307
# OpenAI: gpt-4-turbo-preview, gpt-4o, gpt-4o-mini
# Google: gemini-1.5-pro, gemini-1.5-flash, gemini-2.0-flash
# LLM_MODEL=claude-sonnet-4-5-20250929

# API Keys (set for your chosen provider)
ANTHROPIC_API_KEY=your-anthropic-api-key
OPENAI_API_KEY=your-openai-api-key
GOOGLE_AI_API_KEY=your-google-ai-api-key

# Optional: Enable detailed LLM logging (default: false, auto-enabled in development)
# ENABLE_LLM_LOGGING=true

# ===================
# Learning Loop Feature Flags (Story E7.1)
# ===================
# Source validation before accepting corrections (default: true)
# LEARNING_SOURCE_VALIDATION_ENABLED=true

# Enable full cascade when source document has errors (HIGH RISK - default: false)
# LEARNING_SOURCE_ERROR_CASCADE_ENABLED=false

# Auto-flag all findings from error document (HIGH RISK - default: false)
# LEARNING_AUTO_FLAG_DOCUMENT_FINDINGS=false

# Regenerate embeddings for corrected findings (default: true)
# LEARNING_AUTO_REEMBED_CORRECTIONS=true

# Sync corrections to Neo4j knowledge graph (default: true)
# LEARNING_NEO4J_SYNC_ENABLED=true

# Adjust confidence scores on validation/rejection (default: true)
# LEARNING_CONFIDENCE_ADJUSTMENT_ENABLED=true

# Detect edit patterns from response edits (default: true)
# LEARNING_PATTERN_DETECTION_ENABLED=true

# ===================
# Manda Processing Service (Document Pipeline)
# ===================
# URL of the manda-processing FastAPI service
MANDA_PROCESSING_API_URL=http://localhost:8000
# API key for authenticating with manda-processing
MANDA_PROCESSING_API_KEY=your-processing-api-key

# ===================
# Redis Cache (Story E13.8)
# ===================
# Upstash Redis REST API configuration for cross-instance caching
# Sign up at: https://console.upstash.com
# Uses REST API for serverless compatibility (no persistent connections)

# Required for Redis caching (falls back to in-memory if not set)
UPSTASH_REDIS_REST_URL=https://your-region.upstash.io
UPSTASH_REDIS_REST_TOKEN=your-redis-token

# Optional: Disable Redis caching entirely (useful for debugging)
# REDIS_CACHE_ENABLED=false

# ===================
# LangGraph Checkpointer (Story E13.9)
# ===================
# Durable workflow state persistence for CIM Builder and Supervisor
# Uses existing DATABASE_URL with Transaction mode (port 6543) for connection pooling
# Falls back to in-memory MemorySaver if DATABASE_URL is not set or connection fails
#
# Note: DATABASE_URL is configured in the "Database Configuration" section above
# Ensure it uses Transaction mode (port 6543) for serverless compatibility:
# DATABASE_URL=postgresql://user:password@db.your-project.pooler.supabase.com:6543/postgres
#
# Checkpoint tables are created by migration: supabase/migrations/00051_langgraph_checkpoints.sql
# Cleanup policy: Checkpoints older than 30 days are deleted by scheduled cleanup
# Run cleanup: npm run cleanup-checkpoints
# Recommended cron: 0 3 * * * npm run cleanup-checkpoints

# ===================
# LangSmith Observability (Story E12.11)
# ===================
# Enable tracing to monitor agent behavior, token usage, and latency
# Sign up at: https://smith.langchain.com (or https://eu.smith.langchain.com for EU)
# Provides: exact token counts, per-step latency, cost tracking, visual debugging

LANGSMITH_TRACING=true
LANGSMITH_API_KEY=your-langsmith-api-key
LANGSMITH_PROJECT=manda-platform
# EU region (default is US: https://api.smith.langchain.com)
LANGSMITH_ENDPOINT=https://eu.api.smith.langchain.com

# Background processing for callbacks (true = non-blocking for dev, false = blocking for serverless)
LANGCHAIN_CALLBACKS_BACKGROUND=true

# ===================
# Application Settings
# ===================
NODE_ENV=development
NEXT_PUBLIC_APP_URL=http://localhost:3000
